{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing the text data & 2. Creating dictionary\n",
    "\n",
    "Using the example codes given in the courseware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir (train_dir)]\n",
    "    all_words = []\n",
    "    for mail in emails:\n",
    "        with open (mail) as m:\n",
    "            for i,line in enumerate (m) :\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "    dictionary = Counter(all_words)\n",
    "    \n",
    "    list_to_remove = list(dictionary.keys())\n",
    "    for item in list_to_remove:\n",
    "        if item.isalpha() == False:\n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order', 1414),\n",
       " ('address', 1293),\n",
       " ('report', 1216),\n",
       " ('mail', 1127),\n",
       " ('send', 1079),\n",
       " ('language', 1072),\n",
       " ('email', 1051),\n",
       " ('program', 1001),\n",
       " ('our', 987),\n",
       " ('list', 935),\n",
       " ('one', 917),\n",
       " ('name', 878),\n",
       " ('receive', 826),\n",
       " ('money', 788),\n",
       " ('free', 762),\n",
       " ('work', 755),\n",
       " ('information', 677),\n",
       " ('business', 654),\n",
       " ('please', 652),\n",
       " ('university', 595),\n",
       " ('us', 564),\n",
       " ('day', 556),\n",
       " ('follow', 544),\n",
       " ('internet', 520),\n",
       " ('over', 511),\n",
       " ('http', 479),\n",
       " ('check', 472),\n",
       " ('call', 469),\n",
       " ('each', 466),\n",
       " ('include', 452),\n",
       " ('com', 448),\n",
       " ('linguistic', 442),\n",
       " ('number', 423),\n",
       " ('want', 420),\n",
       " ('letter', 419),\n",
       " ('need', 418),\n",
       " ('many', 412),\n",
       " ('here', 397),\n",
       " ('market', 395),\n",
       " ('start', 390),\n",
       " ('even', 386),\n",
       " ('fax', 383),\n",
       " ('form', 380),\n",
       " ('most', 377),\n",
       " ('first', 373),\n",
       " ('web', 366),\n",
       " ('service', 363),\n",
       " ('interest', 362),\n",
       " ('software', 352),\n",
       " ('remove', 349),\n",
       " ('read', 347),\n",
       " ('those', 345),\n",
       " ('week', 344),\n",
       " ('every', 332),\n",
       " ('credit', 329),\n",
       " ('ll', 326),\n",
       " ('site', 320),\n",
       " ('much', 318),\n",
       " ('english', 318),\n",
       " ('edu', 318),\n",
       " ('product', 317),\n",
       " ('bulk', 312),\n",
       " ('phone', 311),\n",
       " ('must', 299),\n",
       " ('two', 298),\n",
       " ('offer', 297),\n",
       " ('cost', 294),\n",
       " ('best', 291),\n",
       " ('www', 290),\n",
       " ('computer', 289),\n",
       " ('link', 283),\n",
       " ('state', 279),\n",
       " ('card', 278),\n",
       " ('de', 272),\n",
       " ('home', 268),\n",
       " ('available', 266),\n",
       " ('system', 264),\n",
       " ('message', 260),\n",
       " ('own', 260),\n",
       " ('after', 258),\n",
       " ('question', 257),\n",
       " ('opportunity', 257),\n",
       " ('place', 256),\n",
       " ('pay', 256),\n",
       " ('within', 254),\n",
       " ('million', 254),\n",
       " ('write', 253),\n",
       " ('hour', 253),\n",
       " ('world', 253),\n",
       " ('sell', 251),\n",
       " ('help', 248),\n",
       " ('copy', 248),\n",
       " ('show', 248),\n",
       " ('below', 247),\n",
       " ('same', 242),\n",
       " ('conference', 241),\n",
       " ('file', 238),\n",
       " ('through', 235),\n",
       " ('directory', 234),\n",
       " ('before', 232),\n",
       " ('where', 229),\n",
       " ('easy', 229),\n",
       " ('book', 226),\n",
       " ('try', 226),\n",
       " ('income', 226),\n",
       " ('thank', 225),\n",
       " ('tell', 222),\n",
       " ('today', 220),\n",
       " ('company', 219),\n",
       " ('member', 216),\n",
       " ('word', 215),\n",
       " ('example', 215),\n",
       " ('back', 214),\n",
       " ('different', 213),\n",
       " ('advertise', 213),\n",
       " ('month', 210),\n",
       " ('subject', 209),\n",
       " ('etc', 207),\n",
       " ('ask', 206),\n",
       " ('contact', 206),\n",
       " ('response', 204),\n",
       " ('cash', 204),\n",
       " ('little', 203),\n",
       " ('learn', 203),\n",
       " ('add', 202),\n",
       " ('cd', 200),\n",
       " ('linguist', 198),\n",
       " ('research', 198),\n",
       " ('per', 198),\n",
       " ('type', 197),\n",
       " ('down', 196),\n",
       " ('process', 196),\n",
       " ('anyone', 196),\n",
       " ('again', 196),\n",
       " ('right', 196),\n",
       " ('least', 195),\n",
       " ('is', 195),\n",
       " ('ve', 195),\n",
       " ('special', 194),\n",
       " ('price', 193),\n",
       " ('level', 192),\n",
       " ('provide', 192),\n",
       " ('save', 192),\n",
       " ('re', 190),\n",
       " ('off', 190),\n",
       " ('live', 190),\n",
       " ('require', 188),\n",
       " ('few', 186),\n",
       " ('change', 186),\n",
       " ('line', 186),\n",
       " ('sure', 186),\n",
       " ('financial', 186),\n",
       " ('box', 185),\n",
       " ('post', 185),\n",
       " ('search', 185),\n",
       " ('let', 184),\n",
       " ('ffa', 183),\n",
       " ('never', 181),\n",
       " ('paper', 179),\n",
       " ('thing', 177),\n",
       " ('four', 177),\n",
       " ('teach', 175),\n",
       " ('case', 175),\n",
       " ('life', 175),\n",
       " ('page', 174),\n",
       " ('participate', 173),\n",
       " ('text', 172),\n",
       " ('great', 172),\n",
       " ('really', 171),\n",
       " ('between', 170),\n",
       " ('both', 170),\n",
       " ('net', 170),\n",
       " ('buy', 169),\n",
       " ('success', 167),\n",
       " ('plan', 167),\n",
       " ('part', 166),\n",
       " ('move', 164),\n",
       " ('request', 164),\n",
       " ('instruction', 164),\n",
       " ('simply', 164),\n",
       " ('click', 164),\n",
       " ('package', 162),\n",
       " ('reply', 161),\n",
       " ('less', 160),\n",
       " ('point', 160),\n",
       " ('ever', 160),\n",
       " ('seem', 159),\n",
       " ('discourse', 158),\n",
       " ('several', 158),\n",
       " ('dollar', 158),\n",
       " ('theory', 157),\n",
       " ('method', 157),\n",
       " ('why', 157),\n",
       " ('win', 157),\n",
       " ('step', 157),\n",
       " ('student', 156),\n",
       " ('simple', 156),\n",
       " ('put', 155),\n",
       " ('thousand', 155),\n",
       " ('next', 154),\n",
       " ('important', 154),\n",
       " ('friend', 154),\n",
       " ('ship', 154),\n",
       " ('linguistics', 153),\n",
       " ('rate', 153),\n",
       " ('guarantee', 153),\n",
       " ('legal', 152),\n",
       " ('fact', 150),\n",
       " ('note', 150),\n",
       " ('course', 150),\n",
       " ('floodgate', 150),\n",
       " ('under', 148),\n",
       " ('account', 147),\n",
       " ('talk', 147),\n",
       " ('return', 146),\n",
       " ('since', 146),\n",
       " ('allow', 146),\n",
       " ('school', 145),\n",
       " ('position', 145),\n",
       " ('american', 145),\n",
       " ('keep', 145),\n",
       " ('until', 145),\n",
       " ('result', 144),\n",
       " ('analysis', 143),\n",
       " ('experience', 143),\n",
       " ('present', 143),\n",
       " ('ad', 143),\n",
       " ('study', 142),\n",
       " ('reference', 142),\n",
       " ('remember', 142),\n",
       " ('another', 141),\n",
       " ('better', 141),\n",
       " ('sale', 141),\n",
       " ('speaker', 139),\n",
       " ('city', 139),\n",
       " ('wish', 139),\n",
       " ('problem', 139),\n",
       " ('group', 139),\n",
       " ('believe', 139),\n",
       " ('person', 139),\n",
       " ('bank', 138),\n",
       " ('full', 136),\n",
       " ('vium', 136),\n",
       " ('leave', 136),\n",
       " ('once', 136),\n",
       " ('lot', 135),\n",
       " ('answer', 135),\n",
       " ('print', 135),\n",
       " ('family', 135),\n",
       " ('section', 135),\n",
       " ('international', 134),\n",
       " ('area', 134),\n",
       " ('total', 134),\n",
       " ('complete', 134),\n",
       " ('plus', 134),\n",
       " ('georgetown', 134),\n",
       " ('department', 133),\n",
       " ('date', 133),\n",
       " ('grammar', 133),\n",
       " ('too', 133),\n",
       " ('second', 132),\n",
       " ('office', 132),\n",
       " ('lose', 132),\n",
       " ('bill', 131),\n",
       " ('issue', 131),\n",
       " ('future', 131),\n",
       " ('earn', 131),\n",
       " ('while', 130),\n",
       " ('above', 130),\n",
       " ('further', 129),\n",
       " ('long', 129),\n",
       " ('mean', 127),\n",
       " ('amount', 127),\n",
       " ('ca', 126),\n",
       " ('set', 126),\n",
       " ('join', 126),\n",
       " ('run', 125),\n",
       " ('purchase', 125),\n",
       " ('publish', 124),\n",
       " ('server', 124),\n",
       " ('possible', 123),\n",
       " ('become', 123),\n",
       " ('without', 123),\n",
       " ('nothing', 123),\n",
       " ('adult', 123),\n",
       " ('video', 122),\n",
       " ('still', 121),\n",
       " ('real', 121),\n",
       " ('something', 120),\n",
       " ('base', 120),\n",
       " ('accept', 120),\n",
       " ('already', 120),\n",
       " ('code', 119),\n",
       " ('registration', 118),\n",
       " ('uk', 118),\n",
       " ('sign', 118),\n",
       " ('everyone', 118),\n",
       " ('game', 118),\n",
       " ('support', 117),\n",
       " ('major', 117),\n",
       " ('personal', 117),\n",
       " ('understand', 116),\n",
       " ('fee', 116),\n",
       " ('however', 115),\n",
       " ('someone', 115),\n",
       " ('record', 114),\n",
       " ('speech', 114),\n",
       " ('small', 113),\n",
       " ('visit', 113),\n",
       " ('detail', 113),\n",
       " ('minute', 112),\n",
       " ('bonus', 112),\n",
       " ('mailing', 112),\n",
       " ('discussion', 111),\n",
       " ('able', 110),\n",
       " ('hear', 110),\n",
       " ('decide', 110),\n",
       " ('limit', 110),\n",
       " ('john', 109),\n",
       " ('human', 108),\n",
       " ('job', 108),\n",
       " ('hard', 107),\n",
       " ('guide', 107),\n",
       " ('child', 106),\n",
       " ('alway', 106),\n",
       " ('aol', 105),\n",
       " ('begin', 104),\n",
       " ('rule', 104),\n",
       " ('are', 104),\n",
       " ('tax', 104),\n",
       " ('online', 104),\n",
       " ('exactly', 104),\n",
       " ('additional', 103),\n",
       " ('reason', 103),\n",
       " ('past', 103),\n",
       " ('virtual', 103),\n",
       " ('secret', 103),\n",
       " ('either', 102),\n",
       " ('papers', 102),\n",
       " ('open', 102),\n",
       " ('payment', 101),\n",
       " ('general', 101),\n",
       " ('speak', 101),\n",
       " ('charge', 101),\n",
       " ('actually', 101),\n",
       " ('three', 100),\n",
       " ('verb', 100),\n",
       " ('ye', 100),\n",
       " ('customer', 100),\n",
       " ('teacher', 99),\n",
       " ('application', 99),\n",
       " ('content', 99),\n",
       " ('sound', 99),\n",
       " ('investment', 99),\n",
       " ('break', 98),\n",
       " ('technology', 98),\n",
       " ('access', 98),\n",
       " ('fill', 97),\n",
       " ('happen', 97),\n",
       " ('anywhere', 97),\n",
       " ('immediately', 97),\n",
       " ('profit', 97),\n",
       " ('usa', 96),\n",
       " ('structure', 96),\n",
       " ('washington', 96),\n",
       " ('pass', 96),\n",
       " ('choose', 96),\n",
       " ('ed', 95),\n",
       " ('though', 95),\n",
       " ('knowledge', 95),\n",
       " ('build', 94),\n",
       " ('electronic', 94),\n",
       " ('assume', 94),\n",
       " ('html', 94),\n",
       " ('true', 94),\n",
       " ('anything', 93),\n",
       " ('communication', 93),\n",
       " ('ac', 93),\n",
       " ('acquisition', 93),\n",
       " ('abstract', 93),\n",
       " ('sex', 93),\n",
       " ('feel', 93),\n",
       " ('direct', 92),\n",
       " ('wait', 92),\n",
       " ('consider', 91),\n",
       " ('meet', 91),\n",
       " ('topic', 91),\n",
       " ('material', 91),\n",
       " ('yourself', 91),\n",
       " ('contain', 90),\n",
       " ('hand', 90),\n",
       " ('syntax', 89),\n",
       " ('old', 89),\n",
       " ('datum', 89),\n",
       " ('manual', 89),\n",
       " ('end', 89),\n",
       " ('soon', 89),\n",
       " ('single', 88),\n",
       " ('title', 88),\n",
       " ('standard', 88),\n",
       " ('professional', 88),\n",
       " ('generate', 88),\n",
       " ('ago', 87),\n",
       " ('later', 87),\n",
       " ('claim', 87),\n",
       " ('bring', 87),\n",
       " ('science', 87),\n",
       " ('continue', 87),\n",
       " ('postal', 87),\n",
       " ('create', 87),\n",
       " ('national', 86),\n",
       " ('away', 86),\n",
       " ('envelope', 86),\n",
       " ('participant', 85),\n",
       " ('kind', 85),\n",
       " ('everything', 85),\n",
       " ('top', 84),\n",
       " ('delete', 84),\n",
       " ('matter', 83),\n",
       " ('else', 83),\n",
       " ('feature', 83),\n",
       " ('hundred', 83),\n",
       " ('links', 82),\n",
       " ('development', 81),\n",
       " ('phonology', 81),\n",
       " ('center', 81),\n",
       " ('march', 81),\n",
       " ('stop', 81),\n",
       " ('window', 81),\n",
       " ('orders', 81),\n",
       " ('natural', 80),\n",
       " ('telephone', 80),\n",
       " ('mention', 80),\n",
       " ('involve', 80),\n",
       " ('share', 80),\n",
       " ('express', 80),\n",
       " ('pp', 80),\n",
       " ('currency', 80),\n",
       " ('source', 79),\n",
       " ('session', 79),\n",
       " ('german', 79),\n",
       " ('apply', 79),\n",
       " ('stealth', 79),\n",
       " ('idea', 78),\n",
       " ('history', 78),\n",
       " ('version', 78),\n",
       " ('large', 78),\n",
       " ('far', 78),\n",
       " ('action', 78),\n",
       " ('test', 78),\n",
       " ('ready', 78),\n",
       " ('target', 78),\n",
       " ('yours', 78),\n",
       " ('relate', 77),\n",
       " ('approach', 77),\n",
       " ('advantage', 77),\n",
       " ('car', 77),\n",
       " ('country', 77),\n",
       " ('woman', 77),\n",
       " ('visa', 77),\n",
       " ('dream', 77),\n",
       " ('watch', 77),\n",
       " ('education', 76),\n",
       " ('view', 76),\n",
       " ('tool', 76),\n",
       " ('figure', 76),\n",
       " ('handle', 76),\n",
       " ('man', 75),\n",
       " ('effort', 75),\n",
       " ('risk', 75),\n",
       " ('construction', 74),\n",
       " ('spend', 74),\n",
       " ('chair', 74),\n",
       " ('potential', 74),\n",
       " ('russian', 74),\n",
       " ('chance', 74),\n",
       " ('value', 73),\n",
       " ('high', 73),\n",
       " ('easily', 73),\n",
       " ('hold', 73),\n",
       " ('zip', 73),\n",
       " ('whole', 73),\n",
       " ('debt', 73),\n",
       " ('reports', 73),\n",
       " ('dr', 72),\n",
       " ('house', 72),\n",
       " ('extra', 72),\n",
       " ('current', 72),\n",
       " ('organization', 72),\n",
       " ('turn', 72),\n",
       " ('press', 72),\n",
       " ('prove', 72),\n",
       " ('download', 72),\n",
       " ('offshore', 72),\n",
       " ('law', 71),\n",
       " ('mind', 71),\n",
       " ('reach', 71),\n",
       " ('capitalfm', 71),\n",
       " ('field', 70),\n",
       " ('original', 70),\n",
       " ('class', 70),\n",
       " ('age', 70),\n",
       " ('drive', 70),\n",
       " ('collect', 70),\n",
       " ('music', 70),\n",
       " ('fun', 70),\n",
       " ('always', 70),\n",
       " ('society', 69),\n",
       " ('review', 69),\n",
       " ('street', 69),\n",
       " ('publication', 68),\n",
       " ('discover', 68),\n",
       " ('almost', 68),\n",
       " ('cut', 68),\n",
       " ('enter', 68),\n",
       " ('story', 68),\n",
       " ('york', 68),\n",
       " ('security', 68),\n",
       " ('social', 67),\n",
       " ('around', 67),\n",
       " ('enclose', 67),\n",
       " ('dialect', 67),\n",
       " ('pronoun', 67),\n",
       " ('term', 67),\n",
       " ('effective', 67),\n",
       " ('against', 67),\n",
       " ('dear', 67),\n",
       " ('select', 67),\n",
       " ('format', 67),\n",
       " ('probably', 67),\n",
       " ('advertisement', 67),\n",
       " ('engine', 67),\n",
       " ('millions', 67),\n",
       " ('la', 66),\n",
       " ('successful', 66),\n",
       " ('power', 66),\n",
       " ('hope', 66),\n",
       " ('respond', 66),\n",
       " ('st', 66),\n",
       " ('ny', 66),\n",
       " ('means', 66),\n",
       " ('individual', 66),\n",
       " ('doubt', 66),\n",
       " ('room', 65),\n",
       " ('various', 65),\n",
       " ('vowel', 65),\n",
       " ('rest', 65),\n",
       " ('completely', 65),\n",
       " ('user', 65),\n",
       " ('directly', 64),\n",
       " ('whether', 64),\n",
       " ('yet', 64),\n",
       " ('along', 64),\n",
       " ('inc', 64),\n",
       " ('travel', 64),\n",
       " ('big', 64),\n",
       " ('foreign', 63),\n",
       " ('deal', 63),\n",
       " ('rather', 63),\n",
       " ('native', 63),\n",
       " ('sentence', 63),\n",
       " ('sales', 63),\n",
       " ('quite', 62),\n",
       " ('short', 62),\n",
       " ('register', 62),\n",
       " ('train', 62),\n",
       " ('institute', 62),\n",
       " ('finally', 62),\n",
       " ('appear', 62),\n",
       " ('goal', 62),\n",
       " ('comment', 62),\n",
       " ('together', 62),\n",
       " ('connection', 62),\n",
       " ('mass', 62),\n",
       " ('increase', 62),\n",
       " ('shor', 62),\n",
       " ('david', 61),\n",
       " ('phonetic', 61),\n",
       " ('produce', 61),\n",
       " ('enough', 61),\n",
       " ('id', 61),\n",
       " ('object', 60),\n",
       " ('invite', 60),\n",
       " ('vol', 60),\n",
       " ('raise', 60),\n",
       " ('certain', 60),\n",
       " ('resource', 60),\n",
       " ('miss', 60),\n",
       " ('payable', 59),\n",
       " ('design', 59),\n",
       " ('expect', 59),\n",
       " ('summary', 59),\n",
       " ('main', 59),\n",
       " ('rich', 59),\n",
       " ('freedom', 59),\n",
       " ('huge', 59),\n",
       " ('circular', 58),\n",
       " ('french', 58),\n",
       " ('phenomenon', 58),\n",
       " ('chinese', 58),\n",
       " ('develop', 58),\n",
       " ('quality', 58),\n",
       " ('japanese', 58),\n",
       " ('capital', 58),\n",
       " ('website', 58),\n",
       " ('query', 57),\n",
       " ('attention', 57),\n",
       " ('ability', 57),\n",
       " ('semantic', 57),\n",
       " ('journal', 57),\n",
       " ('perhap', 57),\n",
       " ('america', 57),\n",
       " ('play', 57),\n",
       " ('explain', 57),\n",
       " ('entire', 57),\n",
       " ('mastercard', 57),\n",
       " ('control', 56),\n",
       " ('culture', 56),\n",
       " ('advance', 56),\n",
       " ('domain', 56),\n",
       " ('academic', 56),\n",
       " ('index', 56),\n",
       " ('lead', 56),\n",
       " ('multus', 56),\n",
       " ('happy', 56),\n",
       " ('literature', 55),\n",
       " ('article', 55),\n",
       " ('aspect', 55),\n",
       " ('due', 55),\n",
       " ('po', 55),\n",
       " ('historical', 55),\n",
       " ('model', 55),\n",
       " ('local', 55),\n",
       " ('federal', 55),\n",
       " ('dept', 55),\n",
       " ('low', 55),\n",
       " ('info', 55),\n",
       " ('especially', 55),\n",
       " ('difference', 55),\n",
       " ('associate', 55),\n",
       " ('don', 55),\n",
       " ('reduplication', 54),\n",
       " ('exchange', 54),\n",
       " ('specific', 54),\n",
       " ('mary', 54),\n",
       " ('digital', 54),\n",
       " ('mark', 54),\n",
       " ('six', 54),\n",
       " ('master', 54),\n",
       " ('public', 54),\n",
       " ('loan', 54),\n",
       " ('sexual', 54),\n",
       " ('tm', 54),\n",
       " ('cover', 53),\n",
       " ('correct', 53),\n",
       " ('computational', 53),\n",
       " ('author', 53),\n",
       " ('common', 53),\n",
       " ('suite', 53),\n",
       " ('latest', 53),\n",
       " ('close', 53),\n",
       " ('piece', 53),\n",
       " ('recruit', 53),\n",
       " ('filter', 53),\n",
       " ('european', 52),\n",
       " ('recent', 52),\n",
       " ('spanish', 52),\n",
       " ('project', 52),\n",
       " ('track', 52),\n",
       " ('judgment', 52),\n",
       " ('exist', 52),\n",
       " ('mailer', 52),\n",
       " ('kid', 52),\n",
       " ('seven', 52),\n",
       " ('fresh', 52),\n",
       " ('profitable', 52),\n",
       " ('hotel', 51),\n",
       " ('prepare', 51),\n",
       " ('le', 51),\n",
       " ('volume', 51),\n",
       " ('college', 51),\n",
       " ('often', 51),\n",
       " ('absolutely', 51),\n",
       " ('june', 51),\n",
       " ('january', 51),\n",
       " ('sometime', 51),\n",
       " ('concern', 51),\n",
       " ('deliver', 51),\n",
       " ('error', 51),\n",
       " ('news', 51),\n",
       " ('provider', 51),\n",
       " ('invest', 51),\n",
       " ('deat', 50),\n",
       " ('submit', 50),\n",
       " ('announcement', 50),\n",
       " ('formal', 50),\n",
       " ('locate', 50),\n",
       " ('love', 50),\n",
       " ('direction', 50),\n",
       " ('themselve', 50),\n",
       " ('government', 50),\n",
       " ('discuss', 49),\n",
       " ('couple', 49),\n",
       " ('population', 49),\n",
       " ('grammatical', 49),\n",
       " ('indicate', 49),\n",
       " ('committee', 49),\n",
       " ('speed', 49),\n",
       " ('cognitive', 49),\n",
       " ('century', 49),\n",
       " ('qualify', 49),\n",
       " ('enjoy', 49),\n",
       " ('database', 49),\n",
       " ('compuserve', 49),\n",
       " ('hit', 49),\n",
       " ('hot', 49),\n",
       " ('mlm', 49),\n",
       " ('pragmatic', 48),\n",
       " ('third', 48),\n",
       " ('particle', 48),\n",
       " ('upon', 48),\n",
       " ('half', 48),\n",
       " ('grow', 48),\n",
       " ('category', 48),\n",
       " ('situation', 48),\n",
       " ('californium', 48),\n",
       " ('update', 48),\n",
       " ('technical', 48),\n",
       " ('worldwide', 48),\n",
       " ('refer', 48),\n",
       " ('intelligence', 48),\n",
       " ('imagine', 48),\n",
       " ('club', 48),\n",
       " ('powerful', 48),\n",
       " ('fast', 48),\n",
       " ('goldrush', 48),\n",
       " ('dori', 48),\n",
       " ('context', 47),\n",
       " ('wide', 47),\n",
       " ('modern', 47),\n",
       " ('suggest', 47),\n",
       " ('myself', 47),\n",
       " ('agree', 47),\n",
       " ('distinction', 47),\n",
       " ('morphology', 47),\n",
       " ('hr', 47),\n",
       " ('sort', 47),\n",
       " ('discount', 47),\n",
       " ('htm', 47),\n",
       " ('tel', 46),\n",
       " ('syntactic', 46),\n",
       " ('recently', 46),\n",
       " ('clear', 46),\n",
       " ('longer', 46),\n",
       " ('submission', 46),\n",
       " ('cannot', 46),\n",
       " ('phrase', 46),\n",
       " ('length', 46),\n",
       " ('pick', 46),\n",
       " ('girl', 46),\n",
       " ('excite', 46),\n",
       " ('isp', 46),\n",
       " ('signature', 45),\n",
       " ('wonder', 45),\n",
       " ('unite', 45),\n",
       " ('respondent', 45),\n",
       " ('principle', 45),\n",
       " ('particular', 45),\n",
       " ('basis', 45),\n",
       " ('similar', 45),\n",
       " ('early', 45),\n",
       " ('toll', 45),\n",
       " ('benefit', 45),\n",
       " ('sincerely', 45),\n",
       " ('works', 45),\n",
       " ('boyfriend', 45),\n",
       " ('lunch', 44),\n",
       " ('relative', 44),\n",
       " ('forward', 44),\n",
       " ('instead', 44),\n",
       " ('july', 44),\n",
       " ('sheet', 44),\n",
       " ('road', 44),\n",
       " ('space', 44),\n",
       " ('period', 44),\n",
       " ('delivery', 44),\n",
       " ('yes', 44),\n",
       " ('quickly', 44),\n",
       " ('using', 44),\n",
       " ('tense', 44),\n",
       " ('partner', 44),\n",
       " ('five', 44),\n",
       " ('perfectly', 44),\n",
       " ('ems', 44),\n",
       " ('february', 43),\n",
       " ('propose', 43),\n",
       " ('candidate', 43),\n",
       " ('sample', 43),\n",
       " ('non', 43),\n",
       " ('lexical', 43),\n",
       " ('outside', 43),\n",
       " ('evidence', 43),\n",
       " ('initial', 43),\n",
       " ('quick', 43),\n",
       " ('membership', 43),\n",
       " ('worth', 43),\n",
       " ('private', 43),\n",
       " ('movement', 43),\n",
       " ('luck', 43),\n",
       " ('postage', 43),\n",
       " ('der', 43),\n",
       " ('reduce', 43),\n",
       " ('chechen', 43),\n",
       " ('forget', 42),\n",
       " ('practice', 42),\n",
       " ('commercial', 42),\n",
       " ('deaf', 42),\n",
       " ('canada', 42),\n",
       " ('legitimate', 42),\n",
       " ('comparison', 42),\n",
       " ('workshop', 42),\n",
       " ('industry', 42),\n",
       " ('beautiful', 42),\n",
       " ('chain', 42),\n",
       " ('campaign', 42),\n",
       " ('cent', 42),\n",
       " ('duplicate', 42),\n",
       " ('celebrity', 42),\n",
       " ('effect', 41),\n",
       " ('inquiry', 41),\n",
       " ('actual', 41),\n",
       " ('community', 41),\n",
       " ('paul', 41),\n",
       " ('frank', 41),\n",
       " ('shop', 41),\n",
       " ('technique', 41),\n",
       " ('picture', 41),\n",
       " ('introduce', 41),\n",
       " ('phonological', 41),\n",
       " ('forum', 41),\n",
       " ('enterprise', 41),\n",
       " ('regard', 41),\n",
       " ('act', 41),\n",
       " ('ok', 41),\n",
       " ('truly', 41),\n",
       " ('agency', 41),\n",
       " ('rights', 41),\n",
       " ('paradise', 41),\n",
       " ('alter', 41),\n",
       " ('self', 41),\n",
       " ('corporation', 41),\n",
       " ('amaze', 41),\n",
       " ('unique', 40),\n",
       " ('independent', 40),\n",
       " ('quote', 40),\n",
       " ('evaluation', 40),\n",
       " ('drop', 40),\n",
       " ('theoretical', 40),\n",
       " ('replace', 40),\n",
       " ('universal', 40),\n",
       " ('presentation', 40),\n",
       " ('voice', 40),\n",
       " ('table', 40),\n",
       " ('global', 40),\n",
       " ('document', 40),\n",
       " ('thanks', 40),\n",
       " ('compare', 40),\n",
       " ('stock', 40),\n",
       " ('collection', 40),\n",
       " ('alone', 40),\n",
       " ('org', 40),\n",
       " ('monthly', 40),\n",
       " ('optional', 40),\n",
       " ('marketing', 40),\n",
       " ('september', 39),\n",
       " ('activity', 39),\n",
       " ('night', 39),\n",
       " ('prefer', 39),\n",
       " ('organizer', 39),\n",
       " ('au', 39),\n",
       " ('although', 39),\n",
       " ('disk', 39),\n",
       " ('richard', 39),\n",
       " ('higher', 39),\n",
       " ('achieve', 39),\n",
       " ('treat', 39),\n",
       " ('fund', 39),\n",
       " ('description', 39),\n",
       " ('nor', 39),\n",
       " ('artificial', 39),\n",
       " ('operate', 39),\n",
       " ('choice', 39),\n",
       " ('automatically', 39),\n",
       " ('hello', 39),\n",
       " ('girlfriend', 39),\n",
       " ('north', 38),\n",
       " ('depend', 38),\n",
       " ('maybe', 38),\n",
       " ('tape', 38),\n",
       " ('seminar', 38),\n",
       " ('introduction', 38),\n",
       " ('block', 38),\n",
       " ('beach', 38),\n",
       " ('graphic', 38),\n",
       " ('url', 38),\n",
       " ('except', 38),\n",
       " ('expression', 38),\n",
       " ('promise', 38),\n",
       " ('clean', 38),\n",
       " ('moment', 38),\n",
       " ('retire', 38),\n",
       " ('spam', 38),\n",
       " ('late', 37),\n",
       " ('degree', 37),\n",
       " ('final', 37),\n",
       " ('graduate', 37),\n",
       " ('focus', 37),\n",
       " ('certainly', 37),\n",
       " ('across', 37),\n",
       " ('conversation', 37),\n",
       " ('argument', 37),\n",
       " ('dc', 37),\n",
       " ('hesitate', 37),\n",
       " ('task', 37),\n",
       " ('concept', 37),\n",
       " ('necessary', 37),\n",
       " ('daily', 37),\n",
       " ('co', 37),\n",
       " ('sake', 37),\n",
       " ('insurance', 37),\n",
       " ('proof', 37),\n",
       " ('totally', 37),\n",
       " ('benjamin', 37),\n",
       " ('forever', 37),\n",
       " ('rom', 37),\n",
       " ('color', 37),\n",
       " ('mortgage', 37),\n",
       " ('relax', 37),\n",
       " ('michael', 36),\n",
       " ('sense', 36),\n",
       " ('relationship', 36),\n",
       " ('highly', 36),\n",
       " ('whatever', 36),\n",
       " ('addition', 36),\n",
       " ('deadline', 36),\n",
       " ('south', 36),\n",
       " ('eliminate', 36),\n",
       " ('cc', 36),\n",
       " ('describe', 36),\n",
       " ('symbol', 36),\n",
       " ('guvax', 36),\n",
       " ('interpretation', 36),\n",
       " ('background', 36),\n",
       " ('obviously', 36),\n",
       " ('succeed', 36),\n",
       " ('editor', 36),\n",
       " ('clause', 36),\n",
       " ('pc', 36),\n",
       " ('air', 36),\n",
       " ('header', 36),\n",
       " ('extremely', 36),\n",
       " ('prompt', 36),\n",
       " ('ingush', 36),\n",
       " ('trade', 36),\n",
       " ('competition', 36),\n",
       " ('client', 36),\n",
       " ('addresses', 36),\n",
       " ('resell', 36),\n",
       " ('refund', 36),\n",
       " ('semantics', 35),\n",
       " ('item', 35),\n",
       " ('transfer', 35),\n",
       " ('participation', 35),\n",
       " ('obtain', 35),\n",
       " ('edinburgh', 35),\n",
       " ('inform', 35),\n",
       " ('ten', 35),\n",
       " ('among', 35),\n",
       " ('london', 35),\n",
       " ('synthetic', 35),\n",
       " ('element', 35),\n",
       " ('december', 35),\n",
       " ('separate', 35),\n",
       " ('bottom', 35),\n",
       " ('toward', 35),\n",
       " ('germany', 35),\n",
       " ('regular', 35),\n",
       " ('classify', 35),\n",
       " ('stem', 35),\n",
       " ('ipa', 35),\n",
       " ('multiple', 35),\n",
       " ('release', 35),\n",
       " ('accurately', 35),\n",
       " ('undeliverable', 35),\n",
       " ('cultural', 34),\n",
       " ('currently', 34),\n",
       " ('centre', 34),\n",
       " ('skill', 34),\n",
       " ('suggestion', 34),\n",
       " ('range', 34),\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = make_Dictionary('ling-spam/train-mails')\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Extraction\n",
    "Using the example codes given in the courseware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def extract_features(mail_dir):\n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),3000))\n",
    "    docID = 0\n",
    "    _i = 0\n",
    "    print(len(files))\n",
    "    for fil in files:\n",
    "        _i+=1\n",
    "        with open(fil) as fi:\n",
    "            for i,line in enumerate(fi):\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    for word in words:\n",
    "                        wordID = 0\n",
    "                        for i,d in enumerate(dictionary):\n",
    "                            if d[0] == word:\n",
    "                                wordID = i\n",
    "                                features_matrix[docID,wordID]+=1\n",
    "            docID = docID + 1\n",
    "        print('\\r','done {} files'.format(_i),flush=True,end='')\n",
    "    return features_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702\n",
      " done 702 filesone 39 files"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary of words with its frequency\n",
    "features_matrix = extract_features(\"ling-spam/train-mails\")\n",
    "features_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training the Classifier: Preparing Dataset\n",
    "Using the example codes given in the courseware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare feature vectors per training mail and its labels\n",
    "\n",
    "X_train = features_matrix[:]\n",
    "# Counted the number of normal & spam mails\n",
    "y_train = np.append(np.zeros(351), np.ones(351))\n",
    "\n",
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      " done 260 files"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [17.,  2.,  0., ...,  0.,  0.,  0.]]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare feature vectors per testing mail and its labels\n",
    "\n",
    "X_test = extract_features(\"ling-spam/test-mails\")[:]\n",
    "y_test = np.append(np.zeros(130), np.ones(130))\n",
    "\n",
    "X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training the Classifier: Implementation of the Naive Bayes algorithm\n",
    "\n",
    "NOT using the courseware codes, for that they don't perform well.\n",
    "\n",
    "Partly referred to the codes in the SKlearn GaussianNB Class and online blogs, but mostly original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import math\n",
    "class MyGaussianNB():\n",
    "    def __init__(self):\n",
    "        self.map = {}\n",
    "        self.vars = None\n",
    "        self.means = None\n",
    "        self.epsilon = None\n",
    "        self.smoothing = None\n",
    "        self.class_num = None\n",
    "        self.class_prior= None\n",
    "\n",
    "    def fit(self, X, Y, smoothing = 1e-9):\n",
    "        cnt = collections.Counter(Y)\n",
    "        self.class_num = 0\n",
    "        \n",
    "        for k, v in cnt.items():\n",
    "            self.map[k] = self.class_num\n",
    "            self.class_num += 1\n",
    "        \n",
    "        self.class_prior = np.array([ v / len(Y) for k, v in cnt.items()])\n",
    "        \n",
    "        new_y =np.array([self.map[i] for i in Y])\n",
    "        self.means = np.array([X[new_y == id].mean(axis=0) for id in range(self.class_num)])\n",
    "        \n",
    "        new_y = np.array([self.map[i] for i in Y])\n",
    "        self.vars = np.array([X[new_y == id].var(axis=0) for id in range(self.class_num)])\n",
    "        \n",
    "        self.smoothing = smoothing\n",
    "        self.epsilon = self.smoothing * self.vars.max()\n",
    "        self.vars = self.vars + self.epsilon\n",
    "\n",
    "    def gaussian(self, x, u, var):\n",
    "        return -(x - u) ** 2 / (2 * var) - math.log(math.sqrt(2 * math.pi * var))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        result = 0\n",
    "        \n",
    "        for x in X:\n",
    "            temp = 0.0\n",
    "            dims = len(x)\n",
    "            log_prior = np.log(self.class_prior)\n",
    "            for id in range(self.class_num):\n",
    "                likelihood = np.sum([self.gaussian(x[j], self.means[id][j], self.vars[id][j]) for j in range(dims)])\n",
    "                prob = log_prior[id] + likelihood\n",
    "                if prob > temp:\n",
    "                    temp = prob\n",
    "                    result = id\n",
    "            predictions.append(result)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    # Deprecated implementation\\n    # Derived from courseware modules\\n    # Not performing well\\n\\nclass NaiveBayes():\\n\\n    def fit(self, X, y, smoothing):\\n        y = y.astype(int)\\n        self.X = X\\n        self.y = y\\n        self.classes = np.unique(y)\\n        self.parameters = {}\\n        maxVar = 0\\n        for i, c in enumerate(self.classes):\\n            # Calculate the mean, variance, prior probability of each class\\n            X_Index_c = X[np.where(y == c)]\\n            X_index_c_mean = np.mean(X_Index_c, axis=0, keepdims=True)[0]\\n            X_index_c_var = np.var(X_Index_c, axis=0, keepdims=True)[0]\\n            parameters = {\"mean\": X_index_c_mean, \"var\": X_index_c_var, \"prior\": X_Index_c.shape[0] / X.shape[0]}\\n            if parameters[\"var\"].max() > maxVar:\\n                maxVar = parameters[\"var\"].max()\\n            self.parameters[c] = parameters\\n        # Smoothing\\n        self.epsilon = smoothing * maxVar\\n\\n    def gaussian(self, x, u, var):\\n        newVar = var + self.epsilon\\n        return -(x - u) ** 2 / (2 * newVar) - math.log(math.sqrt(2 * math.pi * newVar))\\n\\n    def predict(self, X):\\n        predictions = []\\n        \\n        for x in X:\\n            # return class with highest probability\\n            dims = len(x)\\n            likelihoods = []\\n            log_class_prior = []\\n            temp = 0.0\\n            result = 0\\n            for id in range(len(self.classes)):\\n                likelihoods = np.sum([self.gaussian(x[j], self.parameters[id][\"mean\"][j], self.parameters[id][\"var\"][j]) for j in range(dims)])\\n                log_class_prior = np.log(self.parameters[id][\"prior\"])\\n                all_pros = log_class_prior + likelihoods\\n\\n                if all_pros > temp:\\n                    temp = all_pros\\n                    result = id\\n            \\n            predictions.append(result)\\n            # Complete code for naive Bayes algorithm\\n        \\n        return predictions'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    # Deprecated implementation\n",
    "    # Derived from courseware modules\n",
    "    # Not performing well\n",
    "\n",
    "class NaiveBayes():\n",
    "\n",
    "    def fit(self, X, y, smoothing):\n",
    "        y = y.astype(int)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = {}\n",
    "        maxVar = 0\n",
    "        for i, c in enumerate(self.classes):\n",
    "            # Calculate the mean, variance, prior probability of each class\n",
    "            X_Index_c = X[np.where(y == c)]\n",
    "            X_index_c_mean = np.mean(X_Index_c, axis=0, keepdims=True)[0]\n",
    "            X_index_c_var = np.var(X_Index_c, axis=0, keepdims=True)[0]\n",
    "            parameters = {\"mean\": X_index_c_mean, \"var\": X_index_c_var, \"prior\": X_Index_c.shape[0] / X.shape[0]}\n",
    "            if parameters[\"var\"].max() > maxVar:\n",
    "                maxVar = parameters[\"var\"].max()\n",
    "            self.parameters[c] = parameters\n",
    "        # Smoothing\n",
    "        self.epsilon = smoothing * maxVar\n",
    "\n",
    "    def gaussian(self, x, u, var):\n",
    "        newVar = var + self.epsilon\n",
    "        return -(x - u) ** 2 / (2 * newVar) - math.log(math.sqrt(2 * math.pi * newVar))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        \n",
    "        for x in X:\n",
    "            # return class with highest probability\n",
    "            dims = len(x)\n",
    "            likelihoods = []\n",
    "            log_class_prior = []\n",
    "            temp = 0.0\n",
    "            result = 0\n",
    "            for id in range(len(self.classes)):\n",
    "                likelihoods = np.sum([self.gaussian(x[j], self.parameters[id][\"mean\"][j], self.parameters[id][\"var\"][j]) for j in range(dims)])\n",
    "                log_class_prior = np.log(self.parameters[id][\"prior\"])\n",
    "                all_pros = log_class_prior + likelihoods\n",
    "\n",
    "                if all_pros > temp:\n",
    "                    temp = all_pros\n",
    "                    result = id\n",
    "            \n",
    "            predictions.append(result)\n",
    "            # Complete code for naive Bayes algorithm\n",
    "        \n",
    "        return predictions'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[128   2]\n",
      " [  5 125]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = MyGaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "result = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Checking the results\n",
    "Deriving accuracy score, recall score, and f1 score from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9730769230769231\n",
      "Recall Score: 0.9615384615384616\n",
      "F1 Score: 0.9727626459143969\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "accuracyScore = accuracy_score(y_test, result)\n",
    "recallScore = recall_score(y_test, result)\n",
    "f1Score = f1_score(y_test, result)\n",
    "\n",
    "print(\"Accuracy Score:\", accuracyScore)\n",
    "print(\"Recall Score:\", recallScore)\n",
    "print(\"F1 Score:\", f1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare with SKlearn\n",
    "\n",
    "My model outperforms the GaussianNB in SKlearn on all three scores. Hence I can conclude that my model performs better.\n",
    "\n",
    "I at first consider that the reason why my model does better is the fortune. Namely, it does better only by chance. the reasons are as follow.\n",
    "\n",
    "The Naive Bayes algorithm does not include randomlized factors. Hence, for the same model, the result is always the same with a constant input.\n",
    "\n",
    "One step further, the Gaussian algorithm part of all Gaussian Naive Bayes algorithms should be the same, according to the definition of Gaussian distribution.\n",
    "\n",
    "Specifically on this case, the input datasets are also the same for both mine and SKlearn's GaussianNB. The smoothing coefficients are identical, too.\n",
    "\n",
    "**Hence, I do not see the point where the difference should come in between the two models.**\n",
    "\n",
    "**Except for one step during the process. That is, finding the term with the largest possibility.** There might be plural (in this case, two) terms with possibilities that are identical or too close to be distinguishable for the program. Hence, the two models might choose different terms, belonging to different classes, leading to different results.\n",
    "\n",
    "Though, in fact, when I inverts the priorities of the two classes to make my model choose another class when there are two identical possibilities, the outcome stays the same.\n",
    "\n",
    "This to some extend disproves my first hypothesis.\n",
    "\n",
    "Then, I can only suppose that there are more source codes in the SKlearn Package which **process the data more** for the model to perform well under more circumstances, which I didn't have time to look into more carefully. This should explain why my model does better on this dataset, as which is a tolerable error on the whole for a package that must be suitable under a large number of situations.\n",
    "\n",
    "Or, simply the **floating point arithmetic error** between these two models is to blame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKlearn:\n",
      "Accuracy Score: 0.9615384615384616\n",
      "Recall Score: 0.9307692307692308\n",
      "F1 Score: 0.9603174603174605\n",
      "My model:\n",
      "Accuracy Score: 0.9730769230769231\n",
      "Recall Score: 0.9615384615384616\n",
      "F1 Score: 0.9727626459143969\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "newAc = accuracy_score(y_test, y_pred)\n",
    "newRe = recall_score(y_test, y_pred)\n",
    "newF1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"SKlearn:\")\n",
    "print(\"Accuracy Score:\", newAc)\n",
    "print(\"Recall Score:\", newRe)\n",
    "print(\"F1 Score:\", newF1)\n",
    "print(\"My model:\")\n",
    "print(\"Accuracy Score:\", accuracyScore)\n",
    "print(\"Recall Score:\", recallScore)\n",
    "print(\"F1 Score:\", f1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Questions\n",
    "\n",
    "1. **Describe another real-world application where the nave Bayes method can be applied.**\n",
    "  - Classifying the major of students basing on the courses they take.\n",
    "\n",
    "2. **What are the strengths of the nave Bayes method; when does it perform well?**\n",
    "  - It is derived from the Classical Probability Spaces, with a stable performance.\n",
    "  - It can deal with incremental tasks, hence is friendly to computing power resources.\n",
    "  - It is easy to implement.\n",
    "\n",
    "  It performs well on datasets that are with independent features, which conform to its basic assumption.\n",
    "\n",
    "3. **What are the weaknesses of the nave Bayes method; when does it perform poorly?**\n",
    "  - It assumes that features of the dataset are indepdent, which usually does not hold under production environments.\n",
    "  - Relies on prior probability, which is often hard to decide.\n",
    "\n",
    "  It performs poorly on datasets that contain strongly related features, and when the prior model is not suitable.\n",
    "\n",
    "4. **What makes the nave Bayes method a good candidate for the classification problem, if you have enough knowledge about the data?**\n",
    "  - If we know that the features in the dataset are likely independent, then the naive Bayes method can be a good candidate for that it is easy to implement, fast to train, accurate on classifying, and stable on performance. All these advantages make it fast, reliable, and cost-effective in production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "119555a93dff57b23a42523c50b0a1fb7786bbd4bd2a7dffcdd643392766f1aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
