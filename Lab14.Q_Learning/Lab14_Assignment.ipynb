{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import imageio\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 1 Create Taxi-v3 üöï  environment \n",
    "Using the API imported from gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  500  possible states\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  6  possible actions\n"
     ]
    }
   ],
   "source": [
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space (the set of possible actions the agent can take) is discrete with **6 actions available üéÆ**:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east\n",
    "- 3: move west\n",
    "- 4: pickup passenger\n",
    "- 5: drop off passenger\n",
    "\n",
    "Reward function üí∞:\n",
    "- -1 per step unless other reward is triggered.\n",
    "- +20 delivering passenger.\n",
    "- -10 executing ‚Äúpickup‚Äù and ‚Äúdrop-off‚Äù actions illegally."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 2  Create the Q-table and initialize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.n\n",
    "\n",
    "# Please complete this initialization in this line\n",
    "Q_table = np.zeros((state_space, action_space))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 3 Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 100000        # ‰∏ÄÂÖ±Áé©Â§öÂ∞ëÂ±ÄÊ∏∏Êàè\n",
    "total_test_episodes = 100     # ÊµãËØï‰∏≠‰∏ÄÂÖ±Ëµ∞Âá†Ê≠•\n",
    "max_steps = 99                # Max steps per episode ÊØè‰∏ÄÂ±ÄÊ∏∏ÊàèÊúÄÂ§öËµ∞Âá†Ê≠•\n",
    "\n",
    "learning_rate = 0.5           # Learning rate\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05           # Minimum exploration probability\n",
    "decay_rate = 0.008            # Exponential decay rate for exploration prob\n",
    "\n",
    "\n",
    "test_seed = [16, 54, 165, 177, 191, 191, 120, 80, 149, 178, 48, 38, 6, 125, 174, 73, 50, 172, 100, 148, 146, 6, 25, 40, 68, 148, 49, 167, 9, 97, 164, 176, 61, 7, 54, 55,\n",
    "             161, 131, 184, 51, 170, 12, 120, 113, 95, 126, 51, 98, 36, 135, 54, 82, 45, 95, 89, 59, 95, 124, 9, 113, 58, 85, 51, 134, 121, 169, 105, 21, 30, 11, 50, 65, 12, 43, 82, 145, 152, 97, 106, 55, 31, 85, 38,\n",
    "             112, 102, 168, 123, 97, 21, 83, 158, 26, 80, 63, 5, 81, 32, 11, 28, 148]  # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 4 Q Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99000: average reward:5.268: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99739/100000 [00:35<00:00, 2927.67it/s]"
     ]
    }
   ],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # Randomly generate a number between 0 and 1\n",
    "  random_int = random.uniform(0,1)\n",
    "  # if random_int > greater than epsilon --> exploitation\n",
    "  if random_int > epsilon:\n",
    "    # Take the action with the highest value given a state\n",
    "    # np.argmax can be useful here\n",
    "    action = np.argmax(Qtable[state])\n",
    "  # else --> exploration\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "  \n",
    "  return action\n",
    "\n",
    "bar = tqdm.tqdm(total=total_episodes)\n",
    "sample_rewards = []\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    sample_reward = 0\n",
    "    while True:\n",
    "        #TODO: Please complete this action selection in this line via the maximum value\n",
    "        action = epsilon_greedy_policy(Q_table, state, epsilon)\n",
    "\n",
    "        # TODO:fetech the new state and reward by gym API\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        # Calculate the reward of this episode\n",
    "        sample_reward += reward\n",
    "\n",
    "        # TODO: Update the Q table\n",
    "        # Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        Q_table[state, action] = Q_table[state, action] + \\\n",
    "            learning_rate * (reward +  gamma * np.max(Q_table[new_state]) - Q_table[state, action])\n",
    "\n",
    "        # Update the state\n",
    "        state = new_state\n",
    "\n",
    "        #store the episode reward\n",
    "        if done == True:\n",
    "            sample_rewards.append(sample_reward)\n",
    "            break\n",
    "    # Reduced exploration probability (due to decreasing uncertainty)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * \\\n",
    "        np.exp(-decay_rate*episode)\n",
    "    # print the average reward over 1000 episodes\n",
    "    if episode % 1000 == 0:\n",
    "        mean_reward = np.mean(sample_rewards)\n",
    "        sample_rewards = []\n",
    "        #print(str(episode)+\": average reward:\" + str(mean_reward))\n",
    "        bar.set_description(\n",
    "            str(episode)+\": average reward:\" + str(mean_reward))\n",
    "    bar.update()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 5 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99000: average reward:5.268: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:35<00:00, 2809.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_reward=7.52 +/- 2.67\n"
     ]
    }
   ],
   "source": [
    "fps = 5\n",
    "bar = tqdm.tqdm(total=total_test_episodes)\n",
    "env.reset()\n",
    "rewards = []\n",
    "images = []\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset(seed=test_seed[episode])\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "        #TODO:action selection\n",
    "        action = np.argmax(Q_table[state][:])\n",
    "        #TODO:fetech the new state and reward by gym API\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        total_rewards += reward\n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "\n",
    "env.close()\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "imageio.mimsave('taxi-v3.gif', [np.array(img)\n",
    "                for i, img in enumerate(images)], fps=fps)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 6 Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('taxi-v3.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseclone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov  4 2022, 16:35:55) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4494550fd2c455d12025b7cfce3597d9eb74249dc2acea6a9c1fae47f4abe40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
