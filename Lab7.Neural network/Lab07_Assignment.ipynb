{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB7 Assignment\n",
    "> The document description are designed by JIa Yanhong in 2022. Oct. 20th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "### Exercise 1 logistic regression (20 points )\n",
    "This exercise uses dataset digit01.csv , which has 13 columns, and the last column is the dependent variable. \n",
    "\n",
    "This part requires you to implement a `logistic regression` using the pytorch framework (defining a logistic regression class that inherits `nn.module`). To test your model, we provide a dataset `digit01.csv` which is in the **datasets folder**. This dataset requires you to divide the training set and the test set by yourself, and it is recommended that 80% of the training set and 20% of the test set be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 0 1 1 0 1 1 1 1 0]\n",
      " [0 1 1 1 0 1 1 0 1 1 1 1 0]\n",
      " [1 1 0 1 0 1 1 0 1 1 1 1 0]\n",
      " [1 1 1 1 0 1 1 0 1 1 1 0 0]\n",
      " [1 1 1 1 0 1 1 0 1 0 1 1 0]\n",
      " [0 0 0 1 1 1 1 0 1 1 1 1 0]\n",
      " [0 0 0 0 1 1 1 0 1 1 1 1 0]\n",
      " [0 0 0 1 1 0 1 0 1 1 1 1 0]\n",
      " [0 0 0 1 1 1 1 0 1 1 1 0 0]\n",
      " [0 0 0 1 1 1 1 0 1 0 1 1 0]\n",
      " [1 1 1 1 0 1 1 1 1 0 0 0 0]\n",
      " [0 1 1 1 0 1 1 1 1 0 0 0 0]\n",
      " [1 1 0 1 0 1 1 1 1 0 0 0 0]\n",
      " [1 1 1 1 0 1 1 1 0 0 0 0 0]\n",
      " [1 1 1 1 0 1 0 1 1 0 0 0 0]\n",
      " [1 0 1 1 0 1 1 0 1 1 1 1 0]\n",
      " [1 1 1 1 0 0 1 0 1 1 1 1 0]\n",
      " [1 1 1 1 0 1 1 0 0 1 1 1 0]\n",
      " [1 1 1 1 0 1 1 0 1 1 0 1 0]\n",
      " [1 1 1 1 0 1 0 0 1 1 1 1 0]\n",
      " [1 1 1 0 0 1 1 0 1 1 1 1 0]\n",
      " [0 0 1 1 0 1 1 0 1 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1 1 1 1 0]\n",
      " [0 1 1 1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 1 1 0 1 1 0 1 0]\n",
      " [0 1 1 1 0 1 0 0 1 1 1 1 0]\n",
      " [0 1 1 0 0 1 1 0 1 1 1 1 0]\n",
      " [1 1 0 1 0 0 1 0 1 1 1 1 0]\n",
      " [1 1 0 1 0 1 1 0 0 1 1 1 0]\n",
      " [1 1 0 1 0 1 1 0 1 1 0 1 0]\n",
      " [1 1 0 1 0 1 0 0 1 1 1 1 0]\n",
      " [1 1 0 0 0 1 1 0 1 1 1 1 0]\n",
      " [0 1 0 0 1 0 0 1 0 0 1 0 1]\n",
      " [1 1 0 0 1 0 0 1 0 0 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 0 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 1 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 0 1 1 1]\n",
      " [1 1 0 0 1 0 0 1 0 1 1 0 1]\n",
      " [1 1 0 0 1 0 0 1 0 0 1 1 1]\n",
      " [1 1 0 0 1 0 0 1 0 1 1 1 1]\n",
      " [0 1 0 0 1 1 0 1 0 0 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 1 0 1 0 1]\n",
      " [1 1 0 0 1 1 0 1 0 0 1 0 1]\n",
      " [1 1 0 0 1 0 0 1 1 0 1 0 1]\n",
      " [0 1 0 0 1 1 0 1 0 1 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 1 1 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 1 1 1 1]\n",
      " [1 1 0 0 1 1 0 1 1 0 1 1 1]\n",
      " [1 1 0 0 1 0 0 1 0 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 0 1 1 1]\n",
      " [1 1 0 1 1 0 0 1 0 0 1 0 1]\n",
      " [1 1 0 0 1 0 1 1 0 0 1 0 1]\n",
      " [1 1 0 1 1 0 1 1 0 1 1 0 1]\n",
      " [1 1 0 0 1 0 0 0 0 0 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 1 0 0 1]\n",
      " [1 0 0 0 1 0 0 1 0 0 1 0 1]\n",
      " [1 0 0 0 1 0 0 1 0 0 0 1 1]\n",
      " [0 1 0 0 0 0 0 1 0 1 1 0 1]\n",
      " [0 1 0 0 1 0 0 0 0 1 1 0 1]\n",
      " [0 0 0 0 1 0 0 1 0 1 1 0 1]\n",
      " [0 0 0 0 1 0 0 1 0 0 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 0 0 0 1]\n",
      " [0 1 0 0 0 1 0 0 1 0 1 0 1]\n",
      " [0 1 0 1 0 0 1 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "data = np.loadtxt(\"datasets/digit01.csv\", dtype='int', delimiter=',')\n",
    "print(data)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Splitting dataset into 80% Training and 20% Testing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: [[1 1 1 1 0 1 1 1 1 0 0 0]\n",
      " [1 1 0 1 0 1 1 0 1 1 0 1]\n",
      " [0 0 0 1 1 1 1 0 1 1 1 0]\n",
      " [1 0 1 1 0 1 1 0 1 1 1 1]\n",
      " [1 1 0 0 1 1 0 1 0 0 1 0]\n",
      " [1 1 0 1 0 0 1 0 1 1 1 1]\n",
      " [0 1 1 0 1 1 0 1 1 0 1 1]\n",
      " [1 1 1 1 0 1 0 1 1 0 0 0]\n",
      " [1 1 0 1 0 1 1 0 1 1 1 1]\n",
      " [1 1 0 1 0 1 0 0 1 1 1 1]\n",
      " [0 0 0 1 1 0 1 0 1 1 1 1]\n",
      " [1 1 0 0 1 0 0 0 0 0 1 0]\n",
      " [1 1 0 0 1 1 0 1 1 0 1 1]\n",
      " [0 0 0 1 1 1 1 0 1 1 1 1]\n",
      " [1 1 0 0 1 0 0 1 0 0 1 0]\n",
      " [1 1 0 1 1 0 0 1 0 0 1 0]\n",
      " [1 1 1 1 0 1 1 0 1 1 0 1]\n",
      " [1 1 0 0 1 0 0 1 0 1 1 1]\n",
      " [0 1 0 0 1 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 1 1 0]\n",
      " [1 1 1 1 0 1 1 0 1 0 1 1]\n",
      " [1 1 0 0 0 1 1 0 1 1 1 1]\n",
      " [1 1 1 1 0 1 1 1 0 0 0 0]\n",
      " [1 1 1 1 0 1 0 0 1 1 1 1]\n",
      " [1 1 0 1 1 0 1 1 0 1 1 0]\n",
      " [0 1 0 0 1 1 0 1 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 1 0 0 1 0]\n",
      " [1 1 1 1 0 1 1 0 0 1 1 1]\n",
      " [0 1 0 0 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 1 1 1 1 0 0 0]\n",
      " [0 1 1 1 0 1 1 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 1 0 0 1 1 1]\n",
      " [0 1 0 0 1 0 0 1 1 0 1 0]\n",
      " [0 1 1 1 0 0 1 0 1 1 1 1]\n",
      " [1 1 1 0 0 1 1 0 1 1 1 1]\n",
      " [0 0 0 0 1 1 1 0 1 1 1 1]\n",
      " [0 1 0 0 0 1 0 0 1 0 1 0]\n",
      " [1 1 1 1 0 1 1 0 1 1 1 0]\n",
      " [0 1 0 0 1 1 0 1 0 1 1 0]\n",
      " [0 1 0 0 1 0 0 1 1 1 1 0]\n",
      " [0 1 1 0 0 1 1 0 1 1 1 1]\n",
      " [0 0 1 1 0 1 1 0 1 1 1 1]\n",
      " [0 1 1 1 0 1 1 0 0 1 1 1]\n",
      " [0 1 1 1 0 1 0 0 1 1 1 1]\n",
      " [1 1 0 0 1 0 1 1 0 0 1 0]\n",
      " [0 1 1 1 0 1 1 0 1 1 1 1]\n",
      " [1 1 1 1 0 0 1 0 1 1 1 1]\n",
      " [0 1 0 0 1 0 0 1 0 0 1 0]\n",
      " [1 1 0 1 0 1 1 1 1 0 0 0]\n",
      " [0 1 0 0 1 0 0 1 0 1 1 0]\n",
      " [1 1 1 1 0 1 1 0 1 1 1 1]]\n",
      "X_test: [[0 1 0 0 1 0 0 1 0 1 1 1]\n",
      " [1 1 0 0 1 0 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 1 0 0 0 1 0]\n",
      " [1 1 0 0 1 0 0 1 0 0 1 1]\n",
      " [1 0 0 0 1 0 0 1 0 0 1 0]\n",
      " [0 0 0 1 1 1 1 0 1 0 1 1]\n",
      " [0 1 0 0 1 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 1 0 1 1 0]\n",
      " [1 1 0 0 1 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 0 0 1 0 0 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 0 1 0]\n",
      " [0 1 0 0 1 0 0 1 0 0 1 1]\n",
      " [1 1 0 0 1 0 0 1 0 0 1 0]]\n",
      "y_train: [0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 1\n",
      " 0 1 1 0 0 0 0 1 0 0 1 0 1 0]\n",
      "y_test: [1 1 1 1 1 0 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "#print(X)\n",
    "#print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=41)\n",
    "print(\"X_train:\", X_train)\n",
    "print(\"X_test:\", X_test)\n",
    "print(\"y_train:\", y_train)\n",
    "print(\"y_test:\", y_test)\n",
    "\n",
    "import  torch\n",
    "X_train, X_test = torch.Tensor(X_train), torch.Tensor(X_test)\n",
    "y_train, y_test = torch.Tensor(y_train), torch.Tensor(y_test)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define a LogisticRegression subclass of nn. Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a LogisticRegression subclass of nn. Module.\n",
    "########### Write Your Code Here ###########\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "     def __init__(self, input_dim, output_dim):\n",
    "         super(LogisticRegression, self).__init__()\n",
    "         self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "     def forward(self, x):\n",
    "         return torch.sigmoid(self.linear(x))\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########### Write Your Code Here ###########\n",
    "epochs = 10000\n",
    "input_dim = 12\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "criterion = torch.nn.BCELoss()\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss = 0.6622\n",
      "epoch: 1000, loss = 0.1502\n",
      "epoch: 2000, loss = 0.0919\n",
      "epoch: 3000, loss = 0.0685\n",
      "epoch: 4000, loss = 0.0554\n",
      "epoch: 5000, loss = 0.0469\n",
      "epoch: 6000, loss = 0.0410\n",
      "epoch: 7000, loss = 0.0365\n",
      "epoch: 8000, loss = 0.0329\n",
      "epoch: 9000, loss = 0.0301\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(torch.squeeze(outputs), y_train) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'epoch: {epoch}, loss = {loss.item():.4f}')\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test)\n",
    "    y_pred = torch.nn.Softmax(dim=1)(logits)\n",
    "    #print(logits)\n",
    "    #print(y_pred)\n",
    "    count = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == y_test[i]:\n",
    "            count += 1\n",
    "    acc = count / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc:.4f}')\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         1\n",
      "         1.0       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.92        13\n",
      "   macro avg       0.46      0.50      0.48        13\n",
      "weighted avg       0.85      0.92      0.89        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2  Handwriting recognition with MLP\n",
    "\n",
    "Like last week's lab , your task in this section is also about recognizing handwritten digits, but you are required to use MLP to complete the exercise. It is recommended that you define an MLP class, which is a subclass of `nn.module`.\n",
    "\n",
    "<font color='red' size=4>Note that your accuracy in this section will directly determine your score.</font>\n",
    "\n",
    "For this exercise we use the `minist` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "labels_path = \"datasets/MNIST/raw/train-labels-idx1-ubyte\"\n",
    "images_path = \"datasets/MNIST/raw/train-images-idx3-ubyte\"\n",
    "with open(labels_path, 'rb') as lbpath:\n",
    "    magic, n = struct.unpack('>II',lbpath.read(8))\n",
    "    train_labels = np.fromfile(lbpath,dtype=np.uint8)\n",
    "with open(images_path, 'rb') as imgpath:\n",
    "    magic, num, rows, cols = struct.unpack('>IIII',imgpath.read(16))\n",
    "    train_images = np.fromfile(imgpath,dtype=np.uint8).reshape(len(train_labels), 784)\n",
    "\n",
    "labels_path = \"datasets/MNIST/raw/t10k-labels-idx1-ubyte\"\n",
    "images_path = \"datasets/MNIST/raw/t10k-images-idx3-ubyte\"\n",
    "with open(labels_path, 'rb') as lbpath:\n",
    "    magic, n = struct.unpack('>II',lbpath.read(8))\n",
    "    test_labels = np.fromfile(lbpath,dtype=np.uint8)\n",
    "with open(images_path, 'rb') as imgpath:\n",
    "    magic, num, rows, cols = struct.unpack('>IIII',imgpath.read(16))\n",
    "    test_images = np.fromfile(imgpath,dtype=np.uint8).reshape(len(test_labels), 784)\n",
    "\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "train_labels = train_labels\n",
    "test_labels = test_labels\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define a MLP subclass of nn. Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "class MLP(torch.nn.Module):\n",
    "  def __init__(self, size, hidden=200, classes=10):\n",
    "    super(MLP, self).__init__()\n",
    "    self.layer1=torch.nn.Linear(size, hidden)\n",
    "    self.relu1=torch.nn.ReLU()\n",
    "    self.layer2=torch.nn.Linear(hidden, classes)\n",
    "    self.relu2=torch.nn.ReLU()\n",
    "\n",
    "  def forward(self, input):\n",
    "    out=self.layer1(input)\n",
    "    out=self.relu1(out)\n",
    "    out=self.layer2(out)\n",
    "    out=self.relu2(out)\n",
    "    return out\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "num_epochs = 15\n",
    "learning_rate = 0.0045\n",
    "\n",
    "models = MLP(784)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "criterions = torch.nn.CrossEntropyLoss()\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "optimizers = torch.optim.SGD(models.parameters(), lr=learning_rate)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3452491760253906\n",
      "2.3025851249694824\n",
      "0.0011016021016985178\n",
      "2.3025851249694824\n",
      "0.0014865073608234525\n",
      "2.3025851249694824\n",
      "epoch: 1, loss = 0.9969\n",
      "0.07321454584598541\n",
      "2.3025851249694824\n",
      "0.00011932138295378536\n",
      "2.3025851249694824\n",
      "0.00019774865359067917\n",
      "2.3025851249694824\n",
      "epoch: 2, loss = 0.8148\n",
      "0.011064130812883377\n",
      "2.3025851249694824\n",
      "7.390948667307384e-06\n",
      "0.51663738489151\n",
      "9.16677454370074e-05\n",
      "0.011759838089346886\n",
      "epoch: 3, loss = 0.5157\n",
      "0.008998436853289604\n",
      "0.0003477922291494906\n",
      "7.629365427419543e-06\n",
      "0.00770264957100153\n",
      "6.067568756407127e-05\n",
      "0.003700672183185816\n",
      "epoch: 4, loss = 0.3170\n",
      "0.0035527939908206463\n",
      "0.00014494798961095512\n",
      "1.5497195136049413e-06\n",
      "0.006106528919190168\n",
      "3.6000557884108275e-05\n",
      "0.0009314250783063471\n",
      "epoch: 5, loss = 0.2958\n",
      "0.006441781297326088\n",
      "5.566918844124302e-05\n",
      "1.1920922133867862e-06\n",
      "0.004911263473331928\n",
      "3.814624506048858e-05\n",
      "0.00038521020906046033\n",
      "epoch: 6, loss = 0.2825\n",
      "0.0063132611103355885\n",
      "2.253030106658116e-05\n",
      "9.536738616588991e-07\n",
      "0.003366875695064664\n",
      "2.5033637939486653e-05\n",
      "0.00022456508304458112\n",
      "epoch: 7, loss = 0.2731\n",
      "0.006931900046765804\n",
      "1.0490362910786644e-05\n",
      "2.3841830625315197e-06\n",
      "0.003683569375425577\n",
      "5.245071224635467e-05\n",
      "0.00018773700867313892\n",
      "epoch: 8, loss = 0.0831\n",
      "0.0037365397438406944\n",
      "5.8412379075889476e-06\n",
      "2.145764938177308e-06\n",
      "0.0019309938652440906\n",
      "0.00011312322021694854\n",
      "5.566918844124302e-05\n",
      "epoch: 9, loss = 0.0423\n",
      "0.007302497513592243\n",
      "1.6689286894688848e-06\n",
      "1.4305104514278355e-06\n",
      "0.002035690238699317\n",
      "0.00010513706365600228\n",
      "3.85038583772257e-05\n",
      "epoch: 10, loss = 0.0339\n",
      "0.004359029233455658\n",
      "5.960462772236497e-07\n",
      "2.3841855067985307e-07\n",
      "0.0013493727892637253\n",
      "6.23445157543756e-05\n",
      "1.9073304429184645e-05\n",
      "epoch: 11, loss = 0.0277\n",
      "0.0069275195710361\n",
      "2.3841855067985307e-07\n",
      "1.1920928244535389e-07\n",
      "0.0013509204145520926\n",
      "8.308542601298541e-05\n",
      "1.2040065485052764e-05\n",
      "epoch: 12, loss = 0.0227\n",
      "0.013250508345663548\n",
      "1.1920928244535389e-07\n",
      "-0.0\n",
      "0.0010413468116894364\n",
      "6.222531374078244e-05\n",
      "7.748573807475623e-06\n",
      "epoch: 13, loss = 0.0186\n",
      "0.014230323024094105\n",
      "-0.0\n",
      "-0.0\n",
      "0.0006992755807004869\n",
      "2.5510462364763953e-05\n",
      "5.245195097813848e-06\n",
      "epoch: 14, loss = 0.0155\n",
      "0.012293165549635887\n",
      "-0.0\n",
      "-0.0\n",
      "0.0005606033373624086\n",
      "5.722029527532868e-06\n",
      "4.172316494077677e-06\n",
      "epoch: 15, loss = 0.0131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########### Write Your Code Here ###########\n",
    "models.train()\n",
    "for epoch in range(num_epochs):\n",
    "    tot_loss = 0\n",
    "    for i in range(len(train_images)):\n",
    "        optimizers.zero_grad()\n",
    "        y_pred = models(torch.from_numpy(train_images[i]).float())\n",
    "        y_test = np.zeros((10), dtype=float)\n",
    "        y_test[train_labels[i]] = 1.0\n",
    "        loss = criterions(y_pred, torch.tensor(y_test))\n",
    "        loss.backward()\n",
    "        optimizers.step()\n",
    "        tot_loss += loss.item()\n",
    "        if i % 10000 == 0:\n",
    "            print(loss.item())\n",
    "    print(f'epoch: {epoch+1}, loss = {tot_loss/len(train_images):.4f}')\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9787\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "y_predicted_cls = []\n",
    "models.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for i in range(len(test_images)):\n",
    "        y_pred  = models(torch.from_numpy(test_images[i]).float())\n",
    "        if torch.argmax(y_pred) == test_labels[i]:\n",
    "            correct += 1\n",
    "        y_predicted_cls.append(torch.argmax(y_pred))\n",
    "    print('Accuracy:', correct / len(test_images))\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       980\n",
      "           1       0.98      0.99      0.99      1135\n",
      "           2       0.98      0.97      0.98      1032\n",
      "           3       0.97      0.98      0.98      1010\n",
      "           4       0.98      0.98      0.98       982\n",
      "           5       0.96      0.99      0.98       892\n",
      "           6       0.99      0.97      0.98       958\n",
      "           7       0.99      0.96      0.98      1028\n",
      "           8       0.98      0.97      0.97       974\n",
      "           9       0.96      0.98      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3  Questions (10 points )\n",
    "1.What's the difference between logistic regression and Perceptron?\n",
    "\n",
    "    - `Their loss functions are different. Logistic Regression is maximizing the MLE, Perceptron is minimizing the sum of the distances of incorrect outputs.`\n",
    "    - `Their activate functions are different. Perceptron uses a step function while Logistic Regression uses a sigmoid funciton.`\n",
    "\n",
    "2.Advantages and disadvantages of neural networks?\n",
    "\n",
    "    `Advantages:`\n",
    "      - `Neural Networks improve their performance largely according to the size of dataset, while traditional algorithms stops improving when the dataset size becomes larger and larger. Hence Neural Networks outperform when the training set is large enough.`\n",
    "      - `Different Neural Networks are friendly to supervised or unsupervised scenarios, nearly all engineering problems can be fit into this framework.`\n",
    "      - `Neural Networks are relatively easy to scale, and can handle large size of features without lossing accuracy.`\n",
    "    `Disadvantages:`\n",
    "      - `Neural Networks are black boxes. Most of them are not very explainable, making them unsuitable for fields where decisions need to be reasonable for humans.`\n",
    "      - `A good Neural Network requires lots of time to train and tune. And for that they are not very explainable, this process can be desperately difficult.`\n",
    "      - `Neural Networks need large size of data. For scenarios with few data, they are not suitable.`\n",
    "\n",
    "3.What is the role of Activation Function in Neural networks?\n",
    "\n",
    "    `Activation Function is responsible for introducing non-linear activations to the model, hence making it more suitable for complex real-life scenarios. Otherwise the Neural Networks are only combinations of linear functions, which can only cut edges with more and more linear functions to approach the curve. Activation Functions make this easier and straight-forward.`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('baseclone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4494550fd2c455d12025b7cfce3597d9eb74249dc2acea6a9c1fae47f4abe40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
